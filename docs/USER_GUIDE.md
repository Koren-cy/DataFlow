# DataFlow 用户指南

欢迎使用DataFlow！本指南将帮助您快速上手并充分利用DataFlow的强大功能。

## 目录

- [快速入门](#快速入门)
- [基础概念](#基础概念)
- [工作流示例](#工作流示例)
- [节点详解](#节点详解)
- [最佳实践](#最佳实践)
- [故障排除](#故障排除)
- [高级技巧](#高级技巧)

## 快速入门

### 第一个工作流：数据读取与展示

让我们从最简单的工作流开始：读取CSV文件并展示数据。

#### 步骤1：准备数据

创建一个简单的CSV文件 `sample_data.csv`：

```csv
name,age,score,category
Alice,25,85.5,A
Bob,30,92.0,B
Charlie,28,78.5,A
Diana,35,88.0,B
Eve,22,95.5,A
```

#### 步骤2：构建工作流

1. **添加ReadCSV节点**
   - 在节点面板中找到 "数学建模/数据采集" 分类
   - 拖拽 "读取CSV文件" 节点到画布
   - 在 "文件路径" 参数中输入CSV文件的完整路径

2. **添加ShowAny节点**
   - 在 "数学建模/可视化" 分类中找到 "以文本显示" 节点
   - 将ReadCSV的输出连接到ShowAny的输入

3. **执行工作流**
   - 点击 "Queue Prompt" 按钮
   - 在ShowAny节点中查看读取的数据

恭喜！您已经完成了第一个DataFlow工作流。

---

## 基础概念

### 节点类型

DataFlow包含四大类节点：

#### 1. 数据采集类
- **ReadCSV**: 读取CSV文件
- 支持自动类型推断和编码检测

#### 2. 数据处理类
- **DropNA**: 删除缺失值
- **DropINF**: 删除无穷值
- **Normalize**: 数据标准化
- **SelectColumns**: 列选择
- **BalanceData**: 数据平衡

#### 3. 机器学习类
- **PCA**: 主成分分析降维
- **UMAP**: 非线性降维

#### 4. 可视化类
- **ShowAny**: 文本展示
- **ShowDOM**: HTML展示
- **ShowPygwalker**: 交互式数据探索
- **ShowStreamlit**: Streamlit应用

### 数据类型

#### DATAFRAME
基于pandas的数据框，是DataFlow的核心数据类型：
- 支持多种数据类型（数值、字符串、日期等）
- 具有行索引和列名
- 支持缺失值处理

#### STRING
文本字符串类型：
- 用于文件路径、列名、参数等
- 支持多行文本输入

#### 其他类型
- **INT**: 整数（维度、数量等）
- **FLOAT**: 浮点数（阈值、比例等）
- **ENUM**: 枚举选择（方法、类型等）

---

## 工作流示例

### 示例1：数据清洗流水线

**目标**: 清洗包含缺失值和异常值的数据

**工作流**:
```
ReadCSV → DropNA → DropINF → ShowAny
```

**步骤详解**:

1. **ReadCSV节点配置**
   ```
   文件路径: "data/raw_data.csv"
   ```

2. **DropNA节点**
   - 自动删除包含NaN值的行
   - 无需额外配置

3. **DropINF节点**
   - 自动删除包含无穷值的行
   - 处理除零错误等产生的inf值

4. **ShowAny节点**
   - 展示清洗后的数据统计信息

**预期结果**: 获得干净的数据集，可用于后续分析

### 示例2：数据标准化与降维

**目标**: 对高维数据进行标准化和PCA降维

**工作流**:
```
ReadCSV → SelectColumns → Normalize → PCA → ShowAny
```

**步骤详解**:

1. **ReadCSV节点**
   ```
   文件路径: "data/high_dim_data.csv"
   ```

2. **SelectColumns节点**
   ```
   选择列: "feature1,feature2,feature3,feature4,feature5"
   ```
   - 选择用于分析的数值特征列

3. **Normalize节点**
   ```
   标准化方法: "Z-Score"
   指定列: "" (留空处理所有列)
   ```
   - 进行零均值单位方差标准化

4. **PCA节点**
   ```
   目标维度: 2
   可视化类型: "全部"
   指定列: "" (留空处理所有列)
   标签列: "category" (如果有分类标签)
   ```
   - 降维到2维便于可视化
   - 生成多种图表

5. **ShowAny节点**
   - 展示降维后的数据

**预期结果**: 
- 获得2维的降维数据
- 自动生成碎石图、散点图、热力图等
- 保存在时间戳命名的文件夹中

### 示例3：不平衡数据处理

**目标**: 处理分类不平衡的数据集

**工作流**:
```
ReadCSV → DropNA → BalanceData → ShowAny
```

**步骤详解**:

1. **ReadCSV节点**
   ```
   文件路径: "data/imbalanced_data.csv"
   ```

2. **DropNA节点**
   - 确保数据完整性

3. **BalanceData节点**
   ```
   目标列: "label"
   平衡方法: "SMOTE"
   ```
   - 使用SMOTE算法平衡数据

4. **ShowAny节点**
   - 查看平衡后的类别分布

**预期结果**: 获得类别平衡的数据集

### 示例4：交互式数据探索

**目标**: 使用Pygwalker进行交互式数据分析

**工作流**:
```
ReadCSV → DropNA → ShowPygwalker
```

**步骤详解**:

1. **ReadCSV节点**
   ```
   文件路径: "data/analysis_data.csv"
   ```

2. **DropNA节点**
   - 清理数据

3. **ShowPygwalker节点**
   ```
   主题: "light"
   ```
   - 启动交互式数据探索界面

**预期结果**: 获得类似Tableau的交互式分析界面

---

## 节点详解

### 数据采集类

#### ReadCSV - 读取CSV文件

**功能**: 读取CSV文件并转换为DataFrame

**参数说明**:
- **文件路径**: CSV文件的完整路径
  - 支持绝对路径和相对路径
  - 自动处理路径中的引号
  - 支持中文路径

**使用技巧**:
- 确保文件编码为UTF-8或GBK
- 第一行应为列名
- 避免列名中包含特殊字符

**常见问题**:
- 文件路径错误：检查路径是否正确
- 编码问题：尝试转换文件编码
- 权限问题：确保文件可读

### 数据处理类

#### DropNA - 删除缺失值

**功能**: 删除包含NaN值的行

**处理逻辑**:
- 检查所有列的缺失值
- 删除任何包含NaN的行
- 保持原始列结构

**适用场景**:
- 数据质量要求较高
- 缺失值比例较小
- 后续算法不支持缺失值

#### DropINF - 删除无穷值

**功能**: 删除包含inf或-inf值的行

**处理逻辑**:
- 检查数值列的无穷值
- 删除包含inf/-inf的行
- 常见于除零运算结果

#### Normalize - 数据标准化

**功能**: 对数值数据进行标准化处理

**标准化方法**:

1. **Z-Score标准化**
   ```
   z = (x - μ) / σ
   ```
   - 结果均值为0，标准差为1
   - 适用于正态分布数据

2. **Min-Max标准化**
   ```
   x_norm = (x - min) / (max - min)
   ```
   - 结果范围在[0,1]之间
   - 适用于有明确边界的数据

3. **Robust标准化**
   ```
   x_robust = (x - median) / IQR
   ```
   - 基于中位数和四分位距
   - 对异常值不敏感

**参数配置**:
- **指定列**: 留空处理所有数值列，或指定特定列
- **标准化方法**: 根据数据分布选择合适方法

#### SelectColumns - 列选择

**功能**: 选择数据的特定列

**使用方法**:
- 在"选择列"参数中输入列名
- 多个列名用逗号分隔
- 列名区分大小写

**示例**:
```
选择列: "age,score,category"
```

#### BalanceData - 数据平衡

**功能**: 处理分类不平衡问题

**平衡方法**:

1. **SMOTE** (推荐)
   - 合成少数类样本
   - 保持数据分布特征
   - 适用于大多数场景

2. **RandomOverSampler**
   - 随机复制少数类样本
   - 简单快速
   - 可能导致过拟合

3. **RandomUnderSampler**
   - 随机删除多数类样本
   - 减少数据量
   - 可能丢失重要信息

4. **ADASYN**
   - 自适应合成采样
   - 关注难分类样本
   - 计算复杂度较高

### 机器学习类

#### PCA - 主成分分析

**功能**: 线性降维和特征提取

**算法原理**:
- 找到数据方差最大的方向
- 将数据投影到主成分空间
- 保留最重要的信息

**参数配置**:
- **目标维度**: 降维后的维度数
  - 通常选择2或3便于可视化
  - 不能超过原始特征数

- **可视化类型**: 选择生成的图表
  - **全部**: 生成所有图表
  - **碎石图**: 主成分贡献度
  - **散点图**: 2D/3D数据分布
  - **热力图**: 特征权重
  - **双标图**: 样本和特征关系

- **指定列**: 参与PCA的特征列
  - 留空处理所有数值列
  - 排除非数值列和标签列

- **标签列**: 用于着色的分类标签
  - 在散点图中用不同颜色表示
  - 不参与PCA计算

**输出文件**:
- 所有图表保存为SVG格式
- 文件夹按时间戳命名
- 支持中文字体显示

**使用建议**:
- 数据预处理：先进行标准化
- 维度选择：查看碎石图确定合适维度
- 结果解释：结合热力图理解主成分含义

#### UMAP - 非线性降维

**功能**: 保持局部和全局结构的非线性降维

**算法特点**:
- 保持数据的拓扑结构
- 适用于复杂的非线性数据
- 计算效率高于t-SNE

**参数配置**:
- **邻居数量**: 控制局部结构保持
  - 较小值：保持局部细节
  - 较大值：保持全局结构
  - 推荐范围：5-50

- **最小距离**: 控制嵌入点间距离
  - 较小值：紧密聚类
  - 较大值：均匀分布
  - 推荐范围：0.01-0.5

**适用场景**:
- 高维数据可视化
- 聚类分析
- 异常检测
- 数据探索

### 可视化类

#### ShowAny - 通用展示

**功能**: 将任意数据转换为文本显示

**支持类型**:
- DataFrame：显示形状和前几行
- 数值：直接显示
- 字符串：原样显示
- 复杂对象：JSON序列化或字符串转换

**使用场景**:
- 调试工作流
- 检查中间结果
- 显示统计信息

#### ShowPygwalker - 交互式探索

**功能**: 提供类似Tableau的数据探索界面

**特性**:
- 拖拽式图表创建
- 多种图表类型
- 实时数据筛选
- 统计分析功能

**使用方法**:
1. 连接DataFrame数据
2. 选择主题（浅色/深色）
3. 在界面中拖拽字段创建图表

**支持的图表**:
- 散点图、折线图、柱状图
- 热力图、箱线图、直方图
- 地理图、树图等

---

## 最佳实践

### 工作流设计原则

#### 1. 模块化设计
- 将复杂任务分解为简单步骤
- 每个节点专注单一功能
- 便于调试和维护

#### 2. 数据流清晰
- 从左到右组织节点
- 使用清晰的连接线
- 避免交叉连接

#### 3. 错误处理
- 在关键步骤添加ShowAny节点检查
- 预处理阶段处理异常数据
- 保留原始数据备份

### 性能优化

#### 1. 数据大小控制
- 及早过滤不需要的数据
- 使用SelectColumns减少列数
- 考虑数据采样

#### 2. 内存管理
- 避免创建过多中间变量
- 及时释放大型对象
- 监控内存使用情况

#### 3. 计算优化
- 选择合适的算法参数
- 利用并行计算能力
- 缓存重复计算结果

### 数据质量保证

#### 1. 数据验证
- 检查数据类型和范围
- 验证必需字段完整性
- 识别异常值和离群点

#### 2. 预处理策略
- 制定缺失值处理策略
- 统一数据格式和编码
- 处理重复数据

#### 3. 结果验证
- 检查输出数据的合理性
- 对比处理前后的统计信息
- 验证算法结果的正确性

---

## 故障排除

### 常见错误及解决方案

#### 1. 文件读取错误

**错误信息**: `FileNotFoundError: [Errno 2] No such file or directory`

**可能原因**:
- 文件路径错误
- 文件不存在
- 权限不足

**解决方案**:
```python
# 检查文件是否存在
import os
file_path = "your/file/path.csv"
if os.path.exists(file_path):
    print("文件存在")
else:
    print("文件不存在")

# 使用绝对路径
file_path = os.path.abspath("data/sample.csv")
```

#### 2. 数据类型错误

**错误信息**: `ValueError: 以下列不是数值类型，无法进行PCA`

**可能原因**:
- 选择了非数值列进行数值计算
- 数据包含字符串或日期类型

**解决方案**:
- 使用SelectColumns选择数值列
- 检查数据类型：`df.dtypes`
- 转换数据类型：`pd.to_numeric()`

#### 3. 内存不足

**错误信息**: `MemoryError`

**可能原因**:
- 数据集过大
- 算法复杂度过高
- 系统内存不足

**解决方案**:
- 数据采样：`df.sample(n=10000)`
- 分块处理：`pd.read_csv(chunksize=1000)`
- 优化数据类型：使用更小的数值类型

#### 4. 维度错误

**错误信息**: `ValueError: 目标维度(5)必须小于特征数量(3)`

**解决方案**:
- 检查数据的实际特征数量
- 调整目标维度参数
- 确保有足够的数值列

#### 5. 缺失值错误

**错误信息**: `ValueError: 目标列中存在缺失值`

**解决方案**:
- 使用DropNA节点预处理
- 或使用填充方法：`df.fillna()`
- 检查缺失值分布：`df.isnull().sum()`

### 调试技巧

#### 1. 使用ShowAny节点
在关键步骤添加ShowAny节点查看数据：
```
ReadCSV → ShowAny → DropNA → ShowAny → PCA
```

#### 2. 检查数据形状
```python
print(f"数据形状: {df.shape}")
print(f"列名: {df.columns.tolist()}")
print(f"数据类型: {df.dtypes}")
```

#### 3. 查看数据统计
```python
print(df.describe())  # 数值列统计
print(df.info())      # 数据信息
print(df.head())      # 前几行数据
```

#### 4. 检查缺失值
```python
print(df.isnull().sum())  # 每列缺失值数量
print(df.isnull().any())  # 每列是否有缺失值
```

---

## 高级技巧

### 复杂工作流设计

#### 1. 并行处理
对同一数据进行多种处理：
```
        ┌─ PCA ─ ShowAny
ReadCSV ┤
        └─ UMAP ─ ShowAny
```

#### 2. 条件分支
根据数据特征选择不同处理路径：
```
ReadCSV → SelectColumns → Normalize → PCA → ShowPygwalker
                      └─ BalanceData → UMAP → ShowAny
```

#### 3. 迭代优化
通过多次调整参数优化结果：
```
# 第一轮：快速探索
ReadCSV → DropNA → PCA(维度=2) → ShowAny

# 第二轮：详细分析
ReadCSV → DropNA → Normalize → PCA(维度=3,全部图表) → ShowPygwalker
```

### 自定义参数策略

#### 1. PCA参数选择
- **维度选择**：先用碎石图确定最佳维度
- **可视化类型**：探索阶段用"全部"，生产环境选择特定类型
- **标签列**：有监督场景提供标签，无监督场景留空

#### 2. UMAP参数调优
- **邻居数量**：
  - 小数据集：5-15
  - 大数据集：15-50
  - 聚类明显：较小值
  - 连续分布：较大值

- **最小距离**：
  - 聚类分析：0.01-0.1
  - 可视化：0.1-0.5
  - 密集分布：较小值
  - 稀疏分布：较大值

#### 3. 标准化方法选择
- **Z-Score**：数据近似正态分布
- **Min-Max**：数据有明确边界
- **Robust**：数据包含异常值

### 数据探索策略

#### 1. 渐进式探索
```
阶段1: ReadCSV → ShowAny (了解数据结构)
阶段2: → DropNA → ShowAny (检查清洗效果)
阶段3: → SelectColumns → ShowAny (确认特征选择)
阶段4: → Normalize → PCA → ShowPygwalker (深入分析)
```

#### 2. 多角度分析
```
# 降维分析
Data → PCA → ShowAny
     → UMAP → ShowAny

# 交互探索
Data → ShowPygwalker

# 统计分析
Data → ShowAny (查看describe()结果)
```

#### 3. 结果对比
使用多个ShowAny节点对比不同处理方法的效果：
```
Original → ShowAny
        → Normalize → ShowAny
        → BalanceData → ShowAny
```

### 性能监控

#### 1. 处理时间监控
- 记录每个节点的执行时间
- 识别性能瓶颈
- 优化关键路径

#### 2. 内存使用监控
- 监控数据大小变化
- 及时释放不需要的数据
- 使用内存友好的数据类型

#### 3. 结果质量评估
- 检查降维后的方差保留比例
- 评估聚类效果
- 验证可视化结果的合理性

---

## 实际应用案例

### 案例1：客户细分分析

**背景**: 电商公司需要对客户进行细分，制定个性化营销策略。

**数据**: 客户购买历史、浏览行为、基本信息

**工作流**:
```
ReadCSV → SelectColumns → DropNA → Normalize → PCA → ShowPygwalker
                                            → UMAP → ShowAny
```

**关键配置**:
- SelectColumns: "age,income,purchase_frequency,avg_order_value"
- Normalize: "Z-Score"
- PCA: 维度=2, 可视化="全部"
- UMAP: 邻居=15, 最小距离=0.1

**预期结果**: 识别不同的客户群体，为营销策略提供依据

### 案例2：金融风险评估

**背景**: 银行需要评估贷款申请的风险等级。

**数据**: 申请人财务信息、信用历史、贷款详情

**工作流**:
```
ReadCSV → DropNA → BalanceData → SelectColumns → Normalize → PCA → ShowAny
```

**关键配置**:
- BalanceData: 目标列="risk_level", 方法="SMOTE"
- SelectColumns: 选择相关财务指标
- Normalize: "Robust" (处理异常值)
- PCA: 维度=3, 标签列="risk_level"

**预期结果**: 平衡的风险数据集，便于后续建模

### 案例3：产品推荐系统

**背景**: 内容平台需要分析用户偏好，改进推荐算法。

**数据**: 用户行为数据、内容特征、交互记录

**工作流**:
```
ReadCSV → SelectColumns → DropINF → Normalize → UMAP → ShowPygwalker
```

**关键配置**:
- SelectColumns: 选择用户特征和行为指标
- Normalize: "Min-Max" (统一量纲)
- UMAP: 邻居=30, 最小距离=0.3

**预期结果**: 发现用户群体和内容偏好模式

---

## 总结

DataFlow为数据科学工作流提供了强大而灵活的解决方案。通过合理组合不同节点，您可以：

1. **高效处理数据**: 从读取到清洗的完整流水线
2. **深入分析数据**: 多种降维和可视化方法
3. **交互式探索**: 直观的数据探索界面
4. **灵活定制**: 根据需求调整参数和流程

### 学习建议

1. **从简单开始**: 先掌握基础工作流，再尝试复杂场景
2. **多做实验**: 尝试不同参数组合，理解其影响
3. **关注细节**: 注意数据质量和预处理的重要性
4. **持续学习**: 关注新功能和最佳实践

### 获取帮助

- **文档**: 查阅API文档了解详细参数
- **社区**: 参与GitHub讨论和问题反馈
- **示例**: 学习更多实际应用案例
- **支持**: 联系开发团队获取技术支持

---

*祝您使用DataFlow愉快！如有问题，欢迎随时反馈。*